{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    def k_fold_cv(self, data, k=5):\\n    # ritorna una lista di score (MSE/accuracy), uno per ogni tentativo\\n        # errore se la rete non è stata fittata --> non si conosce il numero di input e output\\n        if not self.weights:\\n            raise UntrainedError('La rete deve prima essere allenata!')\\n\\n        # calcoliamo la lunghezza di ogni divisione del dataset\\n        divided_data_size = len(data) // k\\n        # np.split divide il dataset a seconda degli indici che gli passiamo nella lista (secondo parametro)\\n        # quindi list_subdata è una lista di un np.ndarray bidimensionali (attributi in colonna, record in riga)\\n        list_subdata = np.split(data, [i * divided_data_size for i in range(1, k)])\\n        error_list = []\\n        for i in range(k):\\n            # splitting in test and train\\n            # il test set è semplicemente l'i-esimo elemento della lista delle porzioni del dataset\\n            test_data = list_subdata[i]\\n            # il training set è la lista dei record presenti in tutti gli altri elementi della lista delle porzioni\\n            train_data = []\\n            for j in range(k):\\n                if j != i:\\n                    for row in list_subdata[j]:\\n                        train_data.append(row)\\n            # splitting in train attributes, train target, test attr and test target\\n            train_x = [np.array(row[:-2]) for row in train_data]\\n            train_y = [np.array(row[-2:]) for row in train_data]\\n            test_x = [np.array(row[:-2]) for row in test_data]\\n            test_y = [np.array(row[-2:]) for row in test_data]\\n            # fit the neural network\\n            self.fit(train_x, train_y)\\n            # calculate test error and append it to the result\\n            test_error = self.score(test_x, test_y)\\n            error_list.append(test_error)\\n        \\n        return error_list\\n\\n    def MonteCarlo_cv(self, data, n_fit=5, test_percentage=0.7):\\n    # ritorna una lista di score, come k-fold CV\\n        # errore se la rete non è stata fittata --> non si conosce il numero di input e output\\n        if not self.weights:\\n            raise UntrainedError('La rete deve prima essere allenata!')\\n\\n        error_list = []\\n        for _ in range(n_fit):\\n            shuffle(data)\\n            # splitting in test and train\\n            n_train = round(len(data) * test_percentage)\\n            train_data = data[:n_train, :]\\n            test_data = data[n_train:, :]\\n            # splitting in train attributes, train target, test attr and test target\\n            train_x = [np.array(row[:-2]) for row in train_data]\\n            train_y = [np.array(row[-2:]) for row in train_data]\\n            test_x = [np.array(row[:-2]) for row in test_data]\\n            test_y = [np.array(row[-2:]) for row in test_data]\\n            # fit the neural network\\n            self.fit(train_x, train_y)\\n            # calculate test error\\n            test_error = self.score(test_x, test_y)\\n            error_list.append(test_error)\\n        \\n        return error_list\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import shuffle, randn\n",
    "import matplotlib.pyplot as plt\n",
    "from math import e\n",
    "from math import exp\n",
    "from math import sqrt\n",
    "from math import tanh\n",
    "from random import uniform\n",
    "from copy import deepcopy\n",
    "\n",
    "class Error(Exception):\n",
    "    pass\n",
    "\n",
    "class InputError(Error):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "class UntrainedError(Error):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "'''\n",
    "FUNZIONI DI ATTIVAZIONE NOTE\n",
    "'''\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + e**(-x))\n",
    "\n",
    "def relu(x):\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    return x\n",
    "\n",
    "def my_tanh(x):\n",
    "    return tanh(x)\n",
    "\n",
    "def der(func):\n",
    "    if func == identity:\n",
    "        return lambda x: 1\n",
    "    if func == sigmoid:\n",
    "        return lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
    "    if func == relu:\n",
    "        return lambda x: 0 if x <= 0 else 1\n",
    "    if func == my_tanh:\n",
    "        return lambda x: 1 - tanh(x)**2\n",
    "\n",
    "# creiamo un dizionario che associa nome della funzione come stringa alla funzione stessa\n",
    "#  cosi quando creiamo la rete neurale in input gli diamo una lista di stringhe di funzioni di attivazione \n",
    "dict_funct = {'sigmoid' : sigmoid, 'tanh': my_tanh, 'relu' : relu}\n",
    "\n",
    "\n",
    "def my_sum(matrix_list1, matrix_list2):\n",
    "    return [matrix_list1[i] + matrix_list2[i] for i in range(len(matrix_list1))]\n",
    "\n",
    "def my_prod(matrix_list1, matrix_list2):\n",
    "    return [matrix_list1[i] * matrix_list2[i] for i in range(len(matrix_list1))]\n",
    "\n",
    "def my_div(matrix_list1, matrix_list2):\n",
    "    return [matrix_list1[i] / matrix_list2[i] for i in range(len(matrix_list1))]\n",
    "\n",
    "def my_sqrt(matrix_list1):\n",
    "    return [np.sqrt(matrix_list1[i]) for i in range(len(matrix_list1))]\n",
    "\n",
    "def my_prod_per_scal(scalar, matrix_list):\n",
    "    return [scalar * matrix for matrix in matrix_list]\n",
    "\n",
    "def my_sum_per_scal(scalar, matrix_list):\n",
    "    return [scalar + matrix for matrix in matrix_list]\n",
    "\n",
    "def norm(matrix_list):\n",
    "    norm = 0\n",
    "    for matrix in matrix_list:\n",
    "        for row in matrix:\n",
    "            for elem in row:\n",
    "                norm += elem**2\n",
    "    return sqrt(norm)\n",
    "\n",
    "def one_of_k(data):\n",
    "    dist_values = np.array([np.unique(data[:, i]) for i in range(data.shape[1])])\n",
    "    new_data = []\n",
    "    First_rec = True\n",
    "    for record in data:\n",
    "        new_record = []\n",
    "        First = True\n",
    "        indice = 0\n",
    "        for attribute in record:\n",
    "            new_attribute = np.zeros(len(dist_values[indice]), dtype=int)\n",
    "            for j in range(len(dist_values[indice])):\n",
    "                if dist_values[indice][j] == attribute:\n",
    "                    new_attribute[j] += 1\n",
    "            if First:\n",
    "                new_record = new_attribute\n",
    "                First = False\n",
    "            else:\n",
    "                new_record = np.concatenate((new_record, new_attribute), axis=0)\n",
    "            indice += 1\n",
    "        if First_rec:\n",
    "            new_data = np.array([new_record])\n",
    "            First_rec = False\n",
    "        else:\n",
    "            new_data = np.concatenate((new_data, np.array([new_record])), axis=0)\n",
    "    return new_data\n",
    "\n",
    "'''\n",
    "Una rete neurale viene rappresentata con una lista di matrici, una per ogni layer diverso\n",
    "dall'input layer. Ogni matrice ha una riga per ogni neurone presente nel layer attuale e una colonna\n",
    "per ogni arco entrante in ciascun nodo e una colonna per il bias, quindi il numero di colonne è uguale\n",
    "al numero di neuroni del layer precedente più 1. Queste matrici vengono inizializzate nel metodo fit, che \n",
    "serve ad allenare la rete, cioè nel primo momento in cui si conoscono le dimensioni dell'input e del'output.\n",
    "La rete neurale viene inizializzata tramite:\n",
    "- hidden_layer: una lista di interi, dove l'i-esimo intero rappresenta il numero di neuroni dell'i-esimo hidden layer;\n",
    "- act_functs: una lista di stringhe, dove l'i-esima stringa rappresenta la funzione di attivazione da utilizzare\n",
    "  nell'i-esimo layer (se la stringa non rappresenta una funzione di attivazione nota: NameError)\n",
    "NB: le funzioni di attivazione sono una in più degli hidden layer\n",
    "- toll: float, minimo valore accettabile come errore\n",
    "- learning_rate: float\n",
    "- max_iter: int, numero massimo di volte in cui si scorre il dataset per calcolare il gradiente della funzione da minimizzare\n",
    "- Lambda: float, rappresenta il parametro utilizzato per la regolarizzazione\n",
    "- n_init: int, numero di volte in cui vengono inizializzati i pesi nel metodo fit \n",
    "'''\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, hidden_layers, act_functs, toll=0.1, learning_rate=0.0005, type_lr='const' , alpha = 0, minibatch_size=None, max_epochs=200, Lambda=0.001,  n_init=5, classification=False, algorithm='ADAM'):\n",
    "        if len(act_functs) != len(hidden_layers):\n",
    "            raise InputError('Numero funzioni attivazione != Numero hidden layers')\n",
    "        self.hidden_layers = hidden_layers\n",
    "        # inizializzazione della lista di funzioni di att partendo dalla lista di stringhe usando dict_funct\n",
    "        # mettendo come ultima funzione di attivazione sigmoid se dobbiamo classificare\n",
    "        global dict_funct\n",
    "        try:\n",
    "            act_functs = [dict_funct[string] for string in act_functs]\n",
    "        except:\n",
    "            raise InputError('Una delle funzioni non è stata riconosciuta, lista funzioni valide: ' + str([func for func in dict_funct.keys()]))\n",
    "        if classification:\n",
    "            act_functs += [sigmoid]\n",
    "        else:\n",
    "            act_functs += [identity]\n",
    "        self.act_functs = act_functs\n",
    "        self.weights = None\n",
    "        self.deltas = None\n",
    "        self.toll = toll\n",
    "        self.learning_rate = None\n",
    "        self.initial_lrate = learning_rate\n",
    "        self.type_lr = type_lr\n",
    "        self.alpha = alpha\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.Lambda = Lambda\n",
    "        self.n_init = n_init\n",
    "        self.algorithm = algorithm\n",
    "\n",
    "    def _forward(self, sample):\n",
    "    # ritorna un np.array con gli output\n",
    "        # nel forward voglio che deltas rappresenti l'output di ogni neurone\n",
    "        self.deltas[0] = sample\n",
    "        input_arr = np.append(sample, 1)\n",
    "        for j in range(len(self.weights) - 1):   # per ogni hidden layer\n",
    "            output_arr = self.weights[j] @ input_arr\n",
    "            self.deltas[j + 1] = output_arr     # ci salviamo i nets\n",
    "            output_arr = np.array([self.act_functs[j](element) for element in output_arr])\n",
    "            input_arr = np.append(output_arr, 1)\n",
    "        # e ora l'output layer\n",
    "        output_arr = self.weights[-1] @ input_arr\n",
    "        self.deltas[-1] = output_arr            # ci salviamo i nets\n",
    "\n",
    "        return np.array([self.act_functs[-1](element) for element in output_arr])\n",
    "\n",
    "    def _backward(self, outputNN, target_arr):\n",
    "    # ritorna una lista di matrici (stessa forma dei layer) con il 'gradiente parziale'\n",
    "        # dobbiamo calcolare la 'derivata parziale' per ogni peso,\n",
    "        # quindi il risultato ha la stessa struttura dei layers; lo copio tanto poi sovrascrivo\n",
    "        result = deepcopy(self.weights)\n",
    "\n",
    "        # prima per l'output layer\n",
    "        error_arr = target_arr - outputNN\n",
    "        # sovrascriviamo i net dell'ultimo layer (salvati in self.deltas[-1]) con i delta\n",
    "        # per farlo ci serve il vettore delle derivate delle funzioni di att calcolate nei net\n",
    "        derF_arr = np.array([der(self.act_functs[-1])(net) for net in self.deltas[-1]])\n",
    "        self.deltas[-1] = error_arr * derF_arr\n",
    "        # il vettore delle derivate parziali relative agli ultimi archi è il prodotto tra il vettore colonna\n",
    "        # dei delta e il vettore riga degli output del layer precedente;\n",
    "        # lo salviamo come ultimo elemento della lista di matrici che rappresenta il 'gradiente parziale'\n",
    "        output_prec = np.array([self.act_functs[-2](net) for net in self.deltas[-2]])\n",
    "        # NB: occhio ai bias\n",
    "        output_prec = np.append(output_prec, 1)\n",
    "        # per fare il prodotto vettore colonna per vettore riga bisogna lavorare ancora un po'\n",
    "        deltas_column_vec = self.deltas[-1].reshape(len(self.deltas[-1]), 1)\n",
    "        output_prec = output_prec.reshape(1, len(output_prec))\n",
    "        result[-1] = deltas_column_vec @ output_prec\n",
    "\n",
    "        # poi per gli hidden layers\n",
    "        for i in range(-2, -len(self.weights) - 1, -1):  # bisogna scorrere i layers al contrario, fino al secondo, cioè -len(self.weights)\n",
    "            # NB: i layer sono len(self.weights) + 1 se contiamo anche l'input layer\n",
    "            # cambia il modo di calcolare i delta, ma serve sempre il vettore delle derivate ecc..\n",
    "            derF_arr = np.array([der(self.act_functs[i])(net) for net in self.deltas[i]])\n",
    "            # e lo dobbiamo moltiplicare con il vettore che in posizione i ha la somma su j di\n",
    "            # delta_neurone_succ_jesimo * peso_arco_tra_i_e_j\n",
    "            # NB: i bias non servono!\n",
    "            weight_matr = self.weights[i + 1][:, :-1]    # cancello la colonna dei bias\n",
    "            deltas_sum = self.deltas[i + 1].reshape((1, len(self.deltas[i + 1]))) @ weight_matr\n",
    "            # deltas sum è un vettore riga, voglio che sia un vettore 1D\n",
    "            deltas_sum = deltas_sum.ravel()\n",
    "            self.deltas[i] = deltas_sum * derF_arr\n",
    "            # e nuovamente dobbiamo fare il prodotto vettore riga per vettore colonna\n",
    "            if i > -len(self.weights):\n",
    "                output_prec = np.array([self.act_functs[i - 1](net) for net in self.deltas[i - 1]])\n",
    "            else:   # se il layer precedente è l'input layer non applico le funzioni di attivazione\n",
    "                output_prec = self.deltas[i-1]\n",
    "            output_prec = np.append(output_prec, 1)\n",
    "            output_prec = output_prec.reshape(1, len(output_prec))\n",
    "            deltas_column_vec = self.deltas[i].reshape(len(self.deltas[i]), 1)\n",
    "            result[i] = deltas_column_vec @ output_prec\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for i in range(len(self.weights)):\n",
    "            for j in range(len(self.weights[i])):\n",
    "                for k in range(len(self.weights[i][j])):\n",
    "                    #self.weights[i][j][k] = uniform(-0.7, 0.7)\n",
    "                    self.weights[i][j][k] = randn() * np.sqrt(1 / len(self.weights[i][j]))\n",
    "\n",
    "    def _update_weights(self, gradient):\n",
    "        # regularization\n",
    "        if self.Lambda != 0:\n",
    "            gradient = my_sum(gradient, my_prod_per_scal(-self.Lambda, self.weights))\n",
    "        # update weights\n",
    "        self.weights = my_sum(self.weights, my_prod_per_scal(self.learning_rate, gradient))\n",
    "        return \n",
    "\n",
    "    def fit(self, train_x, train_y, test_x, test_y):\n",
    "        # creiamo la lista di matrici dei pesi, ora che sappiamo le dimensioni dell'input e dell'output\n",
    "        # creiamo anche i delta, ora che sappiamo il numero di neuroni di ogni layer\n",
    "        layers_list = [train_x.shape[1]] + list(self.hidden_layers) + [train_y.shape[1]]\n",
    "        self.weights = [np.empty((layers_list[i], layers_list[i-1] + 1), dtype='float32') for i in range(1, len(layers_list))]\n",
    "        self.deltas = [np.empty(n_neuron, dtype='float32') for n_neuron in layers_list]\n",
    "\n",
    "        # ora inizia l'algoritmo\n",
    "        if self.algorithm == 'ADAM':\n",
    "            min_error = float('inf')\n",
    "            if self.minibatch_size == None:\n",
    "                self.minibatch_size = train_x.shape[0]\n",
    "            # i pesi vanno inizializzati più volte\n",
    "            # ogni volta che li inizializziamo facciamo ripartire l'algoritmo vero e proprio\n",
    "            # memorizziamo l'errore minimo di ogni tentativo e i pesi migliori\n",
    "            for _ in range(self.n_init):\n",
    "                #print('inizializzazione', n + 1)\n",
    "                curr_error = float('inf')\n",
    "                gradient = [np.zeros_like(layer) for layer in self.weights]\n",
    "                self._init_weights()\n",
    "                self.learning_rate = self.initial_lrate\n",
    "                error_list = []\n",
    "                test_error_list = []\n",
    "                acc_list = []\n",
    "                test_acc_list = []\n",
    "\n",
    "                t = 0\n",
    "                m = [np.zeros_like(layer) for layer in self.weights]\n",
    "                v = [np.zeros_like(layer) for layer in self.weights]\n",
    "                m_ = [np.zeros_like(layer) for layer in self.weights]\n",
    "                v_ = [np.zeros_like(layer) for layer in self.weights] \n",
    "                eps = 10**(-8)\n",
    "                beta1 = 0.9\n",
    "                beta2 = 0.999\n",
    "\n",
    "                for n_epochs in range(self.max_epochs):\n",
    "                    # decremento il learning rate\n",
    "                    # step decay\n",
    "                    if type(self.type_lr) == tuple:\n",
    "                        decay_factor = self.type_lr[0]\n",
    "                        step_size = self.type_lr[1]\n",
    "                        self.learning_rate = self.initial_lrate * (decay_factor ** np.floor(n_epochs / step_size))\n",
    "                    # exponential decay\n",
    "                    if type(self.type_lr) == float:\n",
    "                        self.learning_rate = self.initial_lrate * exp(-(self.type_lr * n_epochs))\n",
    "\n",
    "                    # calcolo del gradiente, sommando tutti i risultati di ogni backprop\n",
    "                    for index, pattern in enumerate(train_x):\n",
    "                        outputNN = self._forward(pattern)\n",
    "                        gradient = my_sum(gradient, self._backward(outputNN, train_y[index]))\n",
    "                        if index != 0 and index % self.minibatch_size == 0:\n",
    "                            if self.Lambda != 0:\n",
    "                                gradient = my_sum(gradient, my_prod_per_scal(-self.Lambda, self.weights))\n",
    "                            t = t + 1\n",
    "                            m = my_sum(my_prod_per_scal(beta1, m), my_prod_per_scal((1 - beta1), gradient))\n",
    "                            v = my_sum(my_prod_per_scal(beta2, v), my_prod_per_scal((1 - beta2), my_prod(gradient, gradient)))\n",
    "                            m_ = my_prod_per_scal(1 / (1 - beta1**t), m)\n",
    "                            v_ = my_prod_per_scal(1 / (1 - beta2**t), v)\n",
    "                            num = my_prod_per_scal(self.learning_rate, m_)\n",
    "                            div = my_sum_per_scal(eps, my_sqrt(v_))\n",
    "                            self.weights = my_sum(self.weights, my_div(num, div))\n",
    "\n",
    "                            gradient = my_prod_per_scal(self.alpha, gradient)\n",
    "                    \n",
    "                    # dopo aver visto tutti i pattern bisogna aggiornare i pesi\n",
    "                    if self.Lambda != 0:\n",
    "                        gradient = my_sum(gradient, my_prod_per_scal(-self.Lambda, self.weights))\n",
    "                    t = t + 1\n",
    "                    m = my_sum(my_prod_per_scal(beta1, m), my_prod_per_scal((1 - beta1), gradient))\n",
    "                    v = my_sum(my_prod_per_scal(beta2, v), my_prod_per_scal((1 - beta2), my_prod(gradient, gradient)))\n",
    "                    m_ = my_prod_per_scal(1 / (1 - beta1**t), m)\n",
    "                    v_ = my_prod_per_scal(1 / (1 - beta2**t), v)\n",
    "                    num = my_prod_per_scal(self.learning_rate, m_)\n",
    "                    div = my_sum_per_scal(eps, my_sqrt(v_))\n",
    "                    self.weights = my_sum(self.weights, my_div(num, div))\n",
    "\n",
    "                    gradient = my_prod_per_scal(0, gradient)\n",
    "\n",
    "\n",
    "                    # calcolo errore\n",
    "                    curr_error = self.score(train_x, train_y)\n",
    "                    curr_test_err = self.score(test_x, test_y)\n",
    "                    #print(curr_error)\n",
    "                    error_list.append(curr_error)\n",
    "                    test_error_list.append(curr_test_err)\n",
    "\n",
    "                    #############\n",
    "                    pred_class = [round(elem[0]) for elem in self.predict(train_x)]\n",
    "                    pred_class_test = [round(elem[0]) for elem in self.predict(test_x)]\n",
    "                    train_acc = 1 - sum([1 if pred_class[i] != train_y[i][0] else 0 for i in range(len(pred_class))]) / len(pred_class)\n",
    "                    test_acc = 1 - sum([1 if pred_class_test[i] != test_y[i][0] else 0 for i in range(len(pred_class_test))]) / len(pred_class_test)\n",
    "                    acc_list.append(train_acc)\n",
    "                    test_acc_list.append(test_acc)\n",
    "                    ##############\n",
    "\n",
    "                    if curr_error < self.toll:\n",
    "                        break\n",
    "\n",
    "                # alla fine dell'allenamento, se abbiamo ottenuto risultati migliori aggiorniamo min_error e best_weights\n",
    "                if curr_error <= min_error:\n",
    "                    min_error = curr_error\n",
    "                    best_weights = deepcopy(self.weights)\n",
    "\n",
    "            self.weights = best_weights\n",
    "            return error_list, n_epochs, test_error_list, acc_list, test_acc_list\n",
    "        \n",
    "        if self.algorithm == 'SGE':\n",
    "            # se minibatch_size = None ---> versione batch dell'algoritmo, quindi minibatch_size = len(train_x)\n",
    "            if self.minibatch_size == None:\n",
    "                self.minibatch_size = train_x.shape[0]\n",
    "\n",
    "            # ora inizia l'algoritmo\n",
    "            min_error = float('inf')\n",
    "            # i pesi vanno inizializzati più volte\n",
    "            # ogni volta che li inizializziamo facciamo ripartire l'algoritmo vero e proprio\n",
    "            # memorizziamo l'errore minimo di ogni tentativo e i pesi migliori\n",
    "            for _ in range(self.n_init):\n",
    "                #print('inizializzazione', n + 1)\n",
    "                curr_error = float('inf')\n",
    "                gradient = [np.zeros_like(layer) for layer in self.weights]\n",
    "                self._init_weights()\n",
    "                error_list = []\n",
    "                test_error_list = []\n",
    "                acc_list = []\n",
    "                test_acc_list = []\n",
    "\n",
    "                for n_epochs in range(self.max_epochs):\n",
    "                    # inizializzo / decremento il learning rate\n",
    "                    # constant\n",
    "                    if type(self.type_lr) == str:\n",
    "                        self.learning_rate = self.initial_lrate\n",
    "                    # step decay\n",
    "                    if type(self.type_lr) == tuple:\n",
    "                        decay_factor = self.type_lr[0]\n",
    "                        step_size = self.type_lr[1]\n",
    "                        self.learning_rate = self.initial_lrate * (decay_factor ** np.floor(n_epochs / step_size))\n",
    "                    # exponential decay\n",
    "                    if type(self.type_lr) == float:\n",
    "                        self.learning_rate = self.initial_lrate * exp(-(self.type_lr * n_epochs))\n",
    "\n",
    "                    # calcolo del gradiente, sommando tutti i risultati di ogni backprop\n",
    "                    for index, pattern in enumerate(train_x):\n",
    "                        outputNN = self._forward(pattern)\n",
    "                        gradient = my_sum(gradient, self._backward(outputNN, train_y[index]))\n",
    "                        # dopo minibatch_size passi aggiorniamo i pesi e reinizializziamo il gradiente\n",
    "                        # NB: la regolarizzazione viene fatta in update_weights\n",
    "                        if index != 0 and index % self.minibatch_size == 0:\n",
    "                            self._update_weights(gradient)\n",
    "                            # reset the gradient, to 0 if no momentum(alpha = 0)\n",
    "                            # to alpha times the old gradient otherwise\n",
    "                            gradient = my_prod_per_scal(self.alpha, gradient)\n",
    "\n",
    "                    # dopo aver visto tutti i pattern bisogna nuovamente aggiornare i pesi\n",
    "                    self._update_weights(gradient)\n",
    "                    gradient = my_prod_per_scal(self.alpha, gradient)\n",
    "                    # calcolo errore\n",
    "                    curr_error = self.score(train_x, train_y)\n",
    "                    curr_test_err = self.score(test_x, test_y)\n",
    "                    #print(curr_error)\n",
    "                    error_list.append(curr_error)\n",
    "                    test_error_list.append(curr_test_err)\n",
    "\n",
    "                    #############\n",
    "                    pred_class = [round(elem[0]) for elem in self.predict(train_x)]\n",
    "                    pred_class_test = [round(elem[0]) for elem in self.predict(test_x)]\n",
    "                    train_acc = 1 - sum([1 if pred_class[i] != train_y[i][0] else 0 for i in range(len(pred_class))]) / len(pred_class)\n",
    "                    test_acc = 1 - sum([1 if pred_class_test[i] != test_y[i][0] else 0 for i in range(len(pred_class_test))]) / len(pred_class_test)\n",
    "                    acc_list.append(train_acc)\n",
    "                    test_acc_list.append(test_acc)\n",
    "                    ##############\n",
    "\n",
    "                    if curr_error < self.toll:\n",
    "                        break\n",
    "\n",
    "                # alla fine dell'allenamento, se abbiamo ottenuto risultati migliori aggiorniamo min_error e best_weights\n",
    "                if curr_error <= min_error:\n",
    "                    min_error = curr_error\n",
    "                    best_weights = deepcopy(self.weights)\n",
    "\n",
    "            self.weights = best_weights\n",
    "            return error_list, n_epochs, test_error_list, acc_list, test_acc_list\n",
    "        \n",
    "    def predict(self, data):\n",
    "    # ritorna una lista di np.array con gli output per ogni pattern\n",
    "        # errore se la rete non è stata fittata --> non si conosce il numero di input e output\n",
    "        if not self.weights:\n",
    "            raise UntrainedError('La rete deve prima essere allenata!')\n",
    "        \n",
    "        output_arr = []\n",
    "        len_data = data.shape[0]\n",
    "        for i in range(len_data):\n",
    "            outputNN = self._forward(data[i])\n",
    "            output_arr.append(outputNN)\n",
    "        return np.array(output_arr)\n",
    "\n",
    "    def score(self, data_x, data_y):\n",
    "    # ritorna l'accuracy oppure il mean squared error\n",
    "        # errore se la rete non è stata fittata --> non si conosce il numero di input e output\n",
    "        if not self.weights:\n",
    "            raise UntrainedError('La rete deve prima essere allenata!')\n",
    "\n",
    "        predicted_y = self.predict(data_x)\n",
    "        # NB predicted_y è un array, così come data_y\n",
    "        error_list = [sum((prediction - data_y[index])**2) for index, prediction in enumerate(predicted_y)]\n",
    "        return sum(error_list) / len(error_list)\n",
    "'''\n",
    "    def k_fold_cv(self, data, k=5):\n",
    "    # ritorna una lista di score (MSE/accuracy), uno per ogni tentativo\n",
    "        # errore se la rete non è stata fittata --> non si conosce il numero di input e output\n",
    "        if not self.weights:\n",
    "            raise UntrainedError('La rete deve prima essere allenata!')\n",
    "\n",
    "        # calcoliamo la lunghezza di ogni divisione del dataset\n",
    "        divided_data_size = len(data) // k\n",
    "        # np.split divide il dataset a seconda degli indici che gli passiamo nella lista (secondo parametro)\n",
    "        # quindi list_subdata è una lista di un np.ndarray bidimensionali (attributi in colonna, record in riga)\n",
    "        list_subdata = np.split(data, [i * divided_data_size for i in range(1, k)])\n",
    "        error_list = []\n",
    "        for i in range(k):\n",
    "            # splitting in test and train\n",
    "            # il test set è semplicemente l'i-esimo elemento della lista delle porzioni del dataset\n",
    "            test_data = list_subdata[i]\n",
    "            # il training set è la lista dei record presenti in tutti gli altri elementi della lista delle porzioni\n",
    "            train_data = []\n",
    "            for j in range(k):\n",
    "                if j != i:\n",
    "                    for row in list_subdata[j]:\n",
    "                        train_data.append(row)\n",
    "            # splitting in train attributes, train target, test attr and test target\n",
    "            train_x = [np.array(row[:-2]) for row in train_data]\n",
    "            train_y = [np.array(row[-2:]) for row in train_data]\n",
    "            test_x = [np.array(row[:-2]) for row in test_data]\n",
    "            test_y = [np.array(row[-2:]) for row in test_data]\n",
    "            # fit the neural network\n",
    "            self.fit(train_x, train_y)\n",
    "            # calculate test error and append it to the result\n",
    "            test_error = self.score(test_x, test_y)\n",
    "            error_list.append(test_error)\n",
    "        \n",
    "        return error_list\n",
    "\n",
    "    def MonteCarlo_cv(self, data, n_fit=5, test_percentage=0.7):\n",
    "    # ritorna una lista di score, come k-fold CV\n",
    "        # errore se la rete non è stata fittata --> non si conosce il numero di input e output\n",
    "        if not self.weights:\n",
    "            raise UntrainedError('La rete deve prima essere allenata!')\n",
    "\n",
    "        error_list = []\n",
    "        for _ in range(n_fit):\n",
    "            shuffle(data)\n",
    "            # splitting in test and train\n",
    "            n_train = round(len(data) * test_percentage)\n",
    "            train_data = data[:n_train, :]\n",
    "            test_data = data[n_train:, :]\n",
    "            # splitting in train attributes, train target, test attr and test target\n",
    "            train_x = [np.array(row[:-2]) for row in train_data]\n",
    "            train_y = [np.array(row[-2:]) for row in train_data]\n",
    "            test_x = [np.array(row[:-2]) for row in test_data]\n",
    "            test_y = [np.array(row[-2:]) for row in test_data]\n",
    "            # fit the neural network\n",
    "            self.fit(train_x, train_y)\n",
    "            # calculate test error\n",
    "            test_error = self.score(test_x, test_y)\n",
    "            error_list.append(test_error)\n",
    "        \n",
    "        return error_list\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer = [70, 70], funzioni = ['tanh', 'tanh'], learning_rate = 0.05, Lambda = 0.1, alpha = 0, minibatch_size = 256\n",
      "train error = 1.4906869034508676 test_error = 2.5365911847564586 min test err = 2.526759428740353\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21345320cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eliminiamo la colonna dell'indice\n",
    "data = np.genfromtxt(\"ML-CUP18-TR.csv\", delimiter=',')[:, 1:]\n",
    "# splitting in test and train, after we shuffle the dataset\n",
    "shuffle(data)\n",
    "train_and_val_percentage = 0.7\n",
    "n_train_and_val = round(len(data) * train_and_val_percentage)\n",
    "train_and_val_data = data[:n_train_and_val, :]\n",
    "test_data = data[n_train_and_val:, :]\n",
    "# splitting in train and validation\n",
    "train_percentage = 0.7\n",
    "n_train = round(len(train_and_val_data) * train_percentage)\n",
    "train_data = train_and_val_data[:n_train, :]\n",
    "val_data = train_and_val_data[n_train:, :]\n",
    "# splitting in train attributes, train target, test attr and test target\n",
    "train_x = train_data[:, :-2]\n",
    "train_y = train_data[:, -2:]\n",
    "val_x = val_data[:, :-2]\n",
    "val_y = val_data[:, -2:]\n",
    "# data normalization\n",
    "# Z-score normalization\n",
    "train_x = (train_x - np.mean(train_x, axis=0)) / np.std(train_x, axis=0)\n",
    "val_x = (val_x - np.mean(val_x, axis=0)) / np.std(val_x, axis=0)\n",
    "# normalization in [-1, 1]\n",
    "#for i in range(train_x.shape[1]):\n",
    "#    train_x[:, i] = 2 * ((train_x[:, i] - train_x[:, i].min()) / (train_x[:, i].max() - train_x[:, i].min())) - 1\n",
    "#    val_x[:, i] = 2 * ((val_x[:, i] - val_x[:, i].min()) / (val_x[:, i].max() - val_x[:, i].min())) - 1\n",
    "# grid seach\n",
    "learning_rates = [0.05]\n",
    "lambdas = [0.1]\n",
    "alphas = [0]\n",
    "neurons_per_layer = [70]\n",
    "minibatch_sizes = [256]\n",
    "layers_numbers = [2]\n",
    "type_lr = [0.5, 75]\n",
    "for neuron in neurons_per_layer:\n",
    "    for layer in layers_numbers:\n",
    "        for learning_rate in learning_rates:\n",
    "            for Lambda in lambdas:\n",
    "                for alpha in alphas:\n",
    "                    for  minibatch_size in minibatch_sizes:\n",
    "                        titolo = 'layer = ' + str(layer * [neuron]) + ', funzioni = ' + str((layer) * ['tanh']) + ', learning_rate = ' + str(learning_rate) + ', Lambda = ' + str(Lambda) + ', alpha = ' + str(alpha) + ', minibatch_size = ' + str(minibatch_size)\n",
    "                        print(titolo)\n",
    "                        NN = NeuralNetwork((layer) * [neuron], (layer) * ['tanh'], learning_rate=learning_rate, type_lr=type_lr, Lambda=Lambda, alpha=alpha, toll=0.000001, n_init=1, max_epochs=100, minibatch_size=minibatch_size)\n",
    "                        error_list, n_epochs, test_error_list, acc_list, test_acc_list = NN.fit(train_x, train_y, val_x, val_y)\n",
    "                        print('train error = ' + str(error_list[-1]), 'test_error = ' + str(test_error_list[-1]), 'min test err = ' + str(min(test_error_list)))\n",
    "                        plt.plot(range(n_epochs + 1), error_list)\n",
    "                        plt.plot(range(n_epochs + 1), test_error_list, ls='dashed')\n",
    "                        plt.ylim((-0.5, 10))\n",
    "                        plt.legend(['train error', 'test error'])\n",
    "                        plt.title('MSE')\n",
    "                        plt.xlabel('number of epochs')\n",
    "                        plt.ylabel('MSE')\n",
    "                        plt.show()\n",
    "                        #plt.savefig('C:/Users/danie/Desktop/Daniele/Laurea magistrale/Machine Learning/Machine-Learning-project/plot/MSE_' + titolo +'.png')\n",
    "                        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
